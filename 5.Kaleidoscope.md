# Kaleidoscope

Kaleidoscope è un linguaggio a paradigma funzionale. Illustriamo questo modo di pensare il software.
Per scopi didattici aggiungeremo dei costrutti di tipo operativo.

Un linguaggio funzionale è uno stile di programmazione in cui non esiste il concetto di memoria. C'è il concetto di
ambiente, quindi gli identificatori vanno pensati direttamente legati agli oggetti.

In un linguaggio tipo il C, una definizione di questo tipo:

```c
int a = 1;
```

è modellata con due funzioni. Una che mappa gli identificatori a una zona di memoria, che viene chiamata _ambiente_,
poi c'è una seconda funzione che mappa locazioni di memoria con valori _store_.

> Un linguaggio imperativo ha sia ambiente che memoria.

Le definizioni modificano l'ambiente, mentre gli assegnamenti modificano lo store. Naturalmente questa cosa vale anche
per oggetti più complessi.

Questo ci dà il vantaggio di poter riutilizzare la memoria, che è anche la più grande fonte di bug.

Nel paradigma funzionale c'è una corrispondenza diretta tra gli identificatori e i valori. Se in Python non esistessero
oggetti non Immutable, come le liste, allora avremmo un linguaggio funzionale puro.

```
a = 1
a = 2
```

in un linguaggio funzionale, l'assegnazione non modifica la zona di memoria identificata da a. Anche in Python succede
questo.

Dal punto di vista modellistico dobbiamo pensare che **non ci sia memoria**, quindi una cosa del genere non la possiamo
fare:

```python
a = "pippo"
a[1] = 'o'
```

Non cambia la memoria, ma cambia l'environment. Non c'è il doppio passaggio come in C.

Ma questa cosa non è pesante dal punto di vista delle risorse? Vero, ma un linguaggio funzionale è estremamente
semplice da debuggare. Non ho l'assegnamento ma espressioni e chiamate a funzione.

Il risultato non può essere salvato in una variabile ma viene passato a un'altra funzione o utilizzato in un'espressione.

> I linguaggi funzionali sono Turing-completi. Posso fare tutto, ma con un altro stile.

Dal punto di vista dell'efficienza è conveniente permettere che le funzioni abbiano dei side-effect. Side-effect e
store sono la causa del 95% degli errori di programmazione.

Facciamo un confronto:

| Stile imperativo | Stile funzionale |
|------------------|------------------|
| `int V=0; for(int i=0; i<n; i++) V = V+X[i];` | `` |

Il paradigma funzionale è ancora utilizzato.

Esempio di serie di Fibonacci in Kaleidoscope:

```kal
def fib(n)
    if x<=1
        then 1
    else fib(n-1) + fib(n-2)
```

tutto è espressione, quindi anche l'if può essere passato come parametro a una funzione.

Per "compilare" il codice in Kaleidoscope, possiamo usare `kalcomp` che ha anche una versione verbose `-v`.
Produce una rappresentazione intermedia (IR) per LLVM. Può essere ancora oggetto di svariate ottimizzazioni.

## Definizione funzioni

Si contrassegnano con `def`. I parametri formali sono separati da spazi e poi abbiamo un'espressione.

Una funzione esterna può essere definita con `extern`.

## Grammatica

Dalla slide 37. Un programma Kaleidoscope può essere un `top` seguito da un `programma` (ricorsivo). $\epsilon$ chiude
la ricorsione e ottengo che un programma è una sequenza di top seguiti da `;`.

Un `top` può essere `def` oppure `external` oppure `expr` oppure un $\epsilon$, quindi posso avere `;;` di seguito.

Una funzione `extern` è identificata dal terminale _extern_ con un `proto` prototipo.

Un `def` è il terminale _def_ seguito da un prototipo e un'espressione.

Manca da dire cos'è un'espressione, la cosa sarà un po' più complicata. L'unica convenzione è che `program` sia la
produzione iniziale.

Per prendere pratica con Kaleidoscope proviamo a scrivere dei programmi che rispettino questa sintassi.

## Parsing top-down

Un parser a discesa ricorsiva è pesante perché fa backtracking. Visto che non possiamo realizzare un parser non
deterministico, realizziamo un parser predittivo.

Andiamo a vedere il primo simbolo non terminale in input (il prossimo simbolo quindi).

Consideriamo la grammtica

$$
A \to aA | bB \\
B \to \epsilon | bB
$$

Parliamo di derivazioni canoniche sinistre.

Nel caso di una grammatica LL (left-left) un parser predittivo può essere agevolmente realizzato. In generale possiamo
considerare grammatiche con k caratteri di lookahead LL(k). Noi lavoriamo su LL(1).

Mettiamoci al generico passo di parsing e aver prodotto una forma di frase (passaggio intermedio della derivazione).
Siccome stiamo facendo derivazioni canoniche sinistra, possiamo descrivere la forma di frase generica i come:

$$
\alpha_i = a_1 a_2 \dots a_k A \beta
$$

Metto in evidenza il primo non terminale, perché sviluppo quello più a sinistra.

Il prossimo simbolo di input lo chiamiamo $a = a_{k+1}$ e dobbiamo svilippare. Se ci sono almeno due produzioni per $A$
quale dobbiamo applicare?

$$
A \to \gamma | \delta
$$

$\gamma$ e $\delta$ possono essere stringhe qualsiasi di terminali. La grammatica non può essere LL(1) se sia partendo
da gamma che delta e applicando un certo numero di riscrittura puoi arrivare a una stringa che comincia per $a$.

Il parser guardando un carattere non sa che produzione sarà giusta.

C'è un'altra condizione da verificare affinché la grammatica sia LL(1), se accade che una delle due parti destre è
nullificabile ovvero

$$
\gamma \Rightarrow^* \epsilon
$$

e

$$
\delta \Rightarrow^* a \delta'
$$

allora $a$ non deve essere il primo elemento derivabile da $\beta$.

Praticamente $a$ non deve mai seguire $A$ in una forma di frase.

## Risolvere

Introduciamo due funzioni: `FIRST` e `FOLLOW` (che serve anche per i parser bottom-up). Sono funzioni che restituiscono
insiemi di simboli. Se $\alpha$ (stringa di terminali e non terminali) è un insieme di simboli, allora:

> $FIRST(\alpha)$ è l'insieme di simboli terminali con cui può iniziare una frase derivabile da $\alpha$.

> $FOLLOW(\alpha)$ è l'insieme di simboli terminali che si possono trovare immediatamente a destra di una derivazione
canonica.

## First

> Se X è un terminale, allora poniamo $FIRST(x) = \{x\}$.
>
> Mentre se X è un non-terminale, allora costruiamo la funzione incrementalmente. $FIRST(X) = \{\ \}$.
>   Consideriamo le produzioni di X. Se esiste la produzione $X \to X_1 \dots X_n$ e risulta che i primi j sono
nullificabili, allora $FIRST(X) = FIRST(X) \cup \{ x \}$.

Nel caso di <external> ho come primo terminale solo `extern`.

## Follow

> Se esiste la produzione $B \to \alpha A \beta$, tutti i terminali in $FIRST(\beta)$ si inseriscono in $FOLLOW(A)$.
>
> Inoltre se $B \to \alpha A$, allora tutti i terminali che stanno in $FOLLOW(B)$ stanno anche in $FOLLOW(A)$.
> Per convenzione si mette anche il simbolo di end of file.

Calcoliamo il follow di E: cerchiamo nelle parti destre dove sta la E. Abbiamo la parentesi chiusa in $FOLLOW(E)$.

Se un follow ne include un altro e viceversa, allora sono uguali.

FOLLOW(A) = {$}

FOLLOW(B) = {$} unito FIRST(C)

FIRST(C) = {$, ...}

allora rimane la seconda regola quindi: FOLLOW(B) = {$} unito FOLLOW(A)

quindi FOLLOW(B) = {$, c}

FOLLOW(C) = FOLLOW(A)

Allora FOLLOW(A) = FOLLOW(B) = {$}.

Siamo in grado di dire formalmente quando una grammatica è LL(1).

> Se ho due produzioni relative allo stesso non terminale, i first devono essere disgiunti per avere una LL(1).

Il payoff di questo sforzo è poter costruire una tabella di parsing, per cui il parser ad ogni passo guarda il prossimo
simbolo in input e il prossimo non terminale da sviluppare. A questo punto sviluppare il parser richiede solo uno stack.

Per verificare che una grammatica sia LL(1), andiamo a calcolare gli insieme first e follow.

La tabella di parsing ha dimensioni: righe quanti sono i non terminali e colonne pari a quanti sono i terminali.

### Algoritmo di calcolo del first

Per ogni produzione consideriamo la parte destra (se è una sola, prendiamo quella): il first è il first del primo non
terminale, ma se nullificabile considero il first del secondo non terminale.

Se la parte destra della produzione è nullificabile, allora si va a vedere il follow del simbolo di testa.

> La grammatica è LL(1) se in una cella, per le regole date, non finiscono due produzioni. Le celle vuote sono invece
condizioni di non riconoscimento della stringa.

Ci sono strumenti per calcolare automaticamente first e follow. Una volta costruita la tabella, il processo di parsing
utilizza un solo stack in aggiunta.

## YACC

> Yet another compiler compiler

La differenza tra parser è essenzialmente quale linguaggio riconoscono. Idealmente vorremmo fornire solo la grammatica
e lasciare al computer il compito di fornirmi un parser che la riconosca.

Ho un programma di controllo a cui cambio la tabella di parsing. Codice perfettamente rientrante (solo per il
riconoscimento). Come la genero? Mi serve uno strumento per calcolare `first` e `follow`. Infine uno strumento che
implementi un algoritmo per costruire la tabella di parsing dai first e follow.

> Mai reinventare la ruota. cit.

L'esercitazione consiste nel prende first e follow a scatola chiusa e automatizzare la procedura detta sopra.

> Non c'è mai niente di gratuito. cit.

1. Come forniamo la grammatica? Decidiamo un formalismo. Scriviamo ogni produzione per riga. I terminali sono racchiusi
dagli apici singoli e i non terminali sono rappresentati da stringhe non delimitate. $\epsilon$ è contrassegnato da
**EPS**.

I non terminali spariscono e sono solo come _variabili d'appoggio_.

Il programma `first_and_follow.py` ci fornisce un calcolatore di `first` e `follow`. Salva questi dati in un file in
formato pickle, che è binario.

`set_grammar.sh` chiama `first_and_follow.py` per calcolare i first e i follow. L'output viene passato a un altro
programma `makeparser.py` che genera un file header con la tabella di parsing.

Compiliamo un parser generico da `pparser.cpp` e il lexer. Infine uniamo tutto producendo l'eseguibile `pparser`.
Predictive parser.

Il sorgente di `pparser` tira dentro `pparser.hpp` che contiene la tabella di parsing.

La rappresentazione di non terminali impiega numeri maggiori o uguali di 256, così da rappresentare i token con i
simboli ASCII senza convertire. I numeri vengono rappresentati con un numero negativo.

Non salviamo il non-terminale di testa, tanto lo conosco perché sono già su quella riga.

> Si economizza su tutto. cit.

`make_parser.py` scrive una funzione per creare la tabella di parsing.
