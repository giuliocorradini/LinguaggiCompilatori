# Parser

L'input è una serie di token, e l'output è una abstract syntax tree.

Fin'ora abbiamo fatto analisi lessicale: definendo e descrivendo il lessico tramite linguaggi regolari e come descrivere
questi tramite espressioni regolari. La base teorica sono gli automi finiti (perché il lexer può funzionare). Software
usato: Flex.

Adesso passiamo all'analisi sintattica. Descriviamo la sintassi dei linguaggi con i linguaggi context-free, descrivibili
tramite gramamtiche context-free. Si useranno Yacc e Biso per fare il parsing.

## Limiti dei linguaggi regolari

Consideriamo un frammento di C

```c
while (a > 0) {a=a-1}
```

che compila perfettamente, mentre:

```c
while (a > 0 {a=a-1}
```

a cui manca una parentesi, non deve compilare.

Consideriamo ora l'espressione per il calcolo di un polinomio con il metodo di Horner (il più efficiente per il calcolo
di un polinomio in un punto): `(((a*x+b)*x+c)*x+d)*e`.

[Regola di Horner](https://it.wikipedia.org/wiki/Regola_di_Horner)

Dobbiamo capire se una sequenza di parentesi è perfettamente bilanciata, oppure no. Il requisito si traduce nella
condizione che nella stringa, in ogni suo punto, il numero di parentesi chiuse devono essere minori o uguali delle
parentesi aperte fino a quel punto.

Un linguaggio regolare non può catturare questo aspetto. Questo è un problema di _capacità espressiva_. Un linguaggio
regolare è esprimibile con un automa, che non ha memoria; se ho bisogno di memoria allora non posso esprimere la
condizione con un linguaggio regolare.

Immaginiamo che un dato linguaggio consenta espressioni artimetiche: numeri, token, operatore + (e per x ma + basta) e
le parentesi; ma che consenta di avere il numero di parentesi aperte e non ancora chiuse al più 2. Questo vincolo,
costante, permette di usare un linguaggio regolare. (Riesco a tenere traccia delle parentesi con un numero finito di
stati).

Proviamo a scrivere un e.r. o un AFD.
%TODO

> Le espressioni regolari non sono lo strumento giusto per descrivere vincoli sintattici.

## Context-free

La classe di linguaggi immediatamente più ricca rispetto alle e.r. sono i linguaggi liberi da contesto. Adesso riusciamo
ad esprimere i vincoli di cui sopra: possiamo descrivere la sintassi dei linguaggi di programmazione con i linguaggi
context-free, ma non tutte le caratteristiche sono esprimibili così.

Per riconoscere se ho già definito una variabile (implicitamente o esplicitamente), non posso usare un linguaggio
context-free. Altre caratteristiche sono che il numero di argomenti sia uguale al numero di parametri formali della
firma della funzione.

Scrivere un linguaggio c.f. è semplice, la maggior parte delle strutture sono c.f. e alcune caratteristiche dipendenti
dal contesto vengono trattate nella fase di analisi semantica.

La grammatica del C è uno standard ISO.

[Rust grammar](https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/grammar.html)

## Grammatica

Una grammatica è un formalismo per definire linguaggi.
È un formalismo generativo (cioè mediante la grammatica è possibile generare tutte e sole le frasi del linguaggio).

Il parser può usare la grammatica in due modi:

- parsing top-down, partendo da un assioma fondamentale della grammatica e derivare la frase. Ad esempio nel C l'assioma
fondamentale è `un-programma :` (può essere).

- parsing bottom-up: si procede a ritroso cercando di ritornare all'assioma base.

> Una grammatica è una quadrupla di elementi $G = (\mathcal{N}, \mathcal{T}, \mathcal{P}, \mathcal{S})$. Dove N è
l'insieme dei simboli non terminali (che giocano un ruolo intermedio strutturale), mentre T è un insieme di simboli
terminali. S è il simbolo iniziale (o assioma) e P è l'insieme delle produzioni, cioè scritture nella forma $X
\to Y$ dove $X, Y \in (\mathcal N \cup \mathcal T).

Le produzioni vengono chiamate regole di riscrittura perché dovunque troviamo X allora possiamo sostituirlo con Y. È
questa la grande capacità espressiva delle grammatiche context-free.

### Struttura delle produzioni

$A \to xB$ oppure $A \to x$ è una grammatica lineare destra, mentre $A \to Bx$ è una grammatica
lineare sinistra.

Un linguaggio libero è $A \to \alpha$, ovvero posso sostituire un non terminale con una sequenza di terminali.

Definita una grammatica (vedi slide 20), come fa a descrivere un linguaggio? È di tipo generativo quindi applico
ripetutamente le regole di riscrittura, detta **derivazione**.

> La derivazione è una sequenza di applicazione delle produzioni che fanno passare dall'assioma a una sequenza di soli
terminali; ottenendo una frase del linguaggio.

Si indica con $\Rightarrow$ perché altrimenti avrei ambiguità con la notazione delle produzioni $\to$. Riusciamo a
caratterizzare l'intero linguaggio? Questa genera `a*b*` e si può dimostrare.

Le grammatiche lineari equivalgono alle espressioni regolari.

> Il linguaggio generato da una grammatica è il linguaggio di tutte le sequenze di terminali che possono essere derivate
dall'assioma iniziale.

Le derivazioni si possono rappresentare tramite albero, non in modo biunivoco. I parser bottom-up sono i più potenti e
usati realmente nei parser dei linguaggi.

## Descrizione

Possiamo descrivere una grammatica in modo succinto descrivendo le produzioni direttamente, che contengono tutti e 4 gli
elementi delle grammatiche.

I simboli non terminali compaiono tutti sulla sinistra, un simbolo terminale non può comparire sia in testa che in coda.

Si può fare un'ulteriore semplificazione introducendo un metasimbolo che permette di raggruppare produzioni con la
stessa testa, ovvero `|`.

### Esempi

Consideriamo un'altra grammatica: $E \to E + E | E \times E | (E) | number$. C'è un solo non terminale: $E$.
Ci sono cinque token: $number$, +, $\times$, (, ).

Una frase è una sequenza di simboli **solo terminali**. Una forma di frase può contenere simboli non terminali. Nella
derivazione sono chiaramente tutte forme di frase fino alla penultima.

Dopo un numero di applicazioni (2) puoi ottenere la frase $number + number$: $E + E \Rightarrow ^* _{G_{expr1}}
\text{number} + \text{number}$.

Voglio derivare $n + n \times (n + n)$, allora faccio

$$
E \Rightarrow E + E \Rightarrow E + E \times E \Rightarrow E + E \times (E) \Rightarrow E + E \times (E + E)
\Rightarrow ^* n + n \times (n + n)
$$

Le derivazioni alternative non sono una cosa buona... la cosa ottimale sarebbe avere una sola derivazione per ogni frase
perché legata al concetto di derivazione (e albero) c'è una prima bozza del significato di una frase.

Posso fare la derivazione con il $\times$. Questa grammatica è ambigua.

La derivazione giusta è quella con il $+$ come derivazione iniziale del parse tree. È un concetto distinto dall'abstract
syntax tree però lo genera, quindi applicando subito la regola $\times$ avrei la moltiplicazione come prima operazione.

Data una grammatica, dimostrare che genera un linguaggio, non è banale e si fa per induzione. Si congettura la forma e
bisogna generare che se una stringa è generata allora ha la particolare forma congetturata e viceversa. Ma questo esula
dai nostri scopi.

## Liberi dal contesto

Un linguaggio si chiama libero (tipica domanda da orale), proprio perché le produzioni, in qualsiasi punto della
derivazione se compare la testa della produzione allora posso sostituirla con la produzione indipendentemente da quello
che sta a destra e quello che sta a sinistra.

Mentre nelle grammatiche context-dependent abbiamo due "carabinieri": $\beta A \gamma \to \beta \alpha \gamma$, cioè
posso riscrivere A con $\alpha$ solo se mi trovo tra $\beta$ e $\gamma$.

Il **contesto** fornisce informazioni sulla definizione della funzione (può essere una stringa molto grande).
Pur avendo le grammatiche context-dependent non si usano, ma si fanno controlli semantici algoritmici. Ragione profonda
per cui non si usano.

Sono tutte cose che possiamo inserire nella definizione sintattica, al costo di complicare inultimente la grammatica.

## Gerarchia di Chomksy

Definisce la progressione delle grammatiche in base alla complessità e alla potenza richiesta, dal tipo 3 al tipo 0.

Una grammatica context-free (tipo 2) richiede solo un'automa a stati finiti e uno stack.

Le grammatiche di tipo 1, dipendenti dal contesto, si riconoscono con automi limitati linearmente, quindi con memoria
ad accesso casuale (posso anche accedere sequenzialmente ma devo poter accedere sempre a tutto).

Infine le grammatiche ricorsive si riconoscono con le macchine di Turing.

[Linguaggio ricorsivo](https://it.wikipedia.org/wiki/Linguaggio_ricorsivo)

## Riconoscere parentesi

Scrivere una grammatica per riconoscere sequenze di parentesi. Ad esempio: `()((()()))`

I simboli terminali sono $\mathcal T = \{(, )\}$.

$$
A \to \epsilon \\
A \to (A) \\
A \to AA
$$

che si può semplificare in

$$
A \to \epsilon | (A)A
$$

Nelle grammatiche reali si usano, ad esempio, le parentesi angolate per identificare i simboli non terminali. Le strighe
esterne alla parentesi angolati vengono considerate come simboli terminali. I simboli non terminali vengono anche
chiamati categorie sintattiche.

Esempio:

```
<comando if> -> if <espressione booleana> {
                    <lista comandi>    
                } |
                if <espressione booleana> {
                    <lista comandi>
                } else {
                    <lista comandi>    
                }
```

Ci sono convenzioni simili a quelle adottate dalle espressioni regolari in forma di Backus-Naur.
Ad esempio `[]` per un'opzionale. Mentre `{}` indica una possibile ripetizione. Ad esempio

```
<lista di comandi> -> <comando>{;<comando>}
```

c'è un clash: se il linguaggio ha le graffe come token, per evitare ambiguità si mettono i token unici tra doppi apici.

### Puzzle

> Fornire una grammatica libera per il linguaggio {a^n b^2n | n >= 0} sull'alfabeto a, b

$$
A -> \epsilon | aAbb
$$

cresce dal centro.

> Si fornisca una derivazione per _if b then if b then a else a_. Su questo si gioca una delle ambiguità più importanti.

$$
S => I => if B then S => if B then I => if B then if B then S else S =*> if B then if B then A else A
=> if b then if b then a else a \\
S => I => if B then S else S =*> if b then if b then a else a
$$

Possiamo avere due derivazioni diverse... l'idea di base del parsing è capire se la frase in input sia derivabile dalla
grammatica. Quello che fa un parser, soprattutto a disesca ricorsiva (top-down), cerca di fare un match coi terminali.
Qual è la produzione che mi garantisce? Se ne ho più di una faccio backtracking.

$$
S \to bABAc \\
A \to aA | \epsilon \\
B \to b | c \\
$$

Overpowered, potrei usare solo una regex.

## Equivalenza di grammatiche

Grammatiche diverse possono generare lo stesso linguaggio, quindi a priori un linguaggio non è descritto da una sola
grammatica.

Ad esempio le due grammatiche seguenti descrivono le espressioni aritmetiche:

$$
E \to E + E | E * E \\
E \to (E) | \text{number}
$$

oppure

$$
E \to E + T | T \\
T \to T \times F | F \\
F \to number | (E)
$$

ma la seconda grammatica forza una precedenza degli operatori, cosa che la prima non fa. Se sbaglio derivazione con la
prima posso ottenere alberi sintattici diversi.

## Albero di parsing

Un _parse tree_ per una grammatica è un albero radicato ed etichettato che soddisfa le proprietà:

- i nodi interni sono etichettati coi simboli non terminali (i nodi interni possono essere in qualche modo sviluppati);

- le foglie sono etichettate con $\epsilon$ (se previsto) oppure con simboli terminali di $\mathcal T$;

- la relazione tra nodo interno e figli è una produzione di $\mathcal P$.

> Un parse tree descrive una derivazione? Sì e no: l'albero non mi dice quale ramo ho sviluppato prima.

Quindi a un parse tree possono corrispondere più derivazioni, che in questo caso non cambiano l'albero perché lasciano
la stessa struttura.

Se vogliamo univocità introduciamo le **derivazioni canoniche**, che possono essere destre o sinistre. La derivazione
canonica destra sviluppa sempre il terminale più a destra mentre quella sinistra sviluppa sempre quello più a sinistra.

Nel parsing top-down ci si concentra su derivazioni sinistre, mentre nei parser bottom-up si usano le derivazioni
canoniche destre.

Fissata una derivazione canonica, abbiamo un solo albero.

> Una grammatica è *ambigua* se fissata una derivazione canonica, possono avere origine alberi diversi.

Due derivazioni canoniche diverse per `n x n + n`:

1.
$$
E => E + E => E + n => E x E + n => n x n + n
$$

2.
$$
E => E x E => E x E + E => n x n + n
$$

quella "giusta" è la seconda perché dà origine all'AST che cerchiamo.

> Parsing: passare da una struttura lineare (priva di struttura) a una struttura bidimensionale (l'albero ha struttura).

Formalmente nei linguaggi di programmazione `else` viene associato all'if più vicino a sé. Quindi anche nell'espressione
`if b then if b then a else a` può avere due interpretazioni, dove associo l'else?

Si può forzare la grammatica a seguire questa regola, invece di gestirla in modo procedurale, in modo più barocco:

$$

$$

## Progettazione

Ci sono aspetti problematici nella progettazione di una grammatica per tipi di parsing.
Le grammatiche possono avere cicli, cioè derivazioni del tipo $A \xRightarrow{+} A$. Ad esempio un parser top-down con
backtracking va in loop, oppure quando ci sono casi di left recursion $A \xRightarrow{+} A \alpha$.

Anche se ho prefissi comuni a due produzioni relative allo stesso non terminale: $A \to \alpha \beta_1$ e $A \to \alpha
\beta_2$. Problema se non per $\alpha = 1$, ma introduco un lookahead.

In alcuni casi per eliminare l'ambiguità basta introdurre altri simboli non terminali.

Proviamo a ottenere ambiguità sempre sulla stessa frase `n x n + n`.

Nelle espressioni aritmetiche c'è anche il problema dell'associatività. Ho bisogno che il linguaggio non cambi
l'associatività.

In matematica è tutto associativo a sinistra, ma l'esponenziale è associativo a destra.

$$
E \to E + T | E - T | T \\
T \to T * P | T / P | P \\
$$

# TODO

Si fornisca una grammatica libera per il linguaggio su B contenente tutte e sole le stringhe con un diverso numero di 0
e 1. Grammatica con uno zero in più di un 1 (è la base).
