# Parser

L'input è una serie di token, e l'output è una abstract syntax tree.

Fin'ora abbiamo fatto analisi lessicale: definendo e descrivendo il lessico tramite linguaggi regolari e come descrivere
questi tramite espressioni regolari. La base teorica sono gli automi finiti (perché il lexer può funzionare). Software
usato: Flex.

Adesso passiamo all'analisi sintattica. Descriviamo la sintassi dei linguaggi con i linguaggi context-free, descrivibili
tramite gramamtiche context-free. Si useranno Yacc e Biso per fare il parsing.

## Limiti dei linguaggi regolari

Consideriamo un frammento di C

```c
while (a > 0) {a=a-1}
```

che compila perfettamente, mentre:

```c
while (a > 0 {a=a-1}
```

a cui manca una parentesi, non deve compilare.

Consideriamo ora l'espressione per il calcolo di un polinomio con il metodo di Horner (il più efficiente per il calcolo
di un polinomio in un punto): `(((a*x+b)*x+c)*x+d)*e`.

[Regola di Horner](https://it.wikipedia.org/wiki/Regola_di_Horner)

Dobbiamo capire se una sequenza di parentesi è perfettamente bilanciata, oppure no. Il requisito si traduce nella
condizione che nella stringa, in ogni suo punto, il numero di parentesi chiuse devono essere minori o uguali delle
parentesi aperte fino a quel punto.

Un linguaggio regolare non può catturare questo aspetto. Questo è un problema di _capacità espressiva_. Un linguaggio
regolare è esprimibile con un automa, che non ha memoria; se ho bisogno di memoria allora non posso esprimere la
condizione con un linguaggio regolare.

Immaginiamo che un dato linguaggio consenta espressioni artimetiche: numeri, token, operatore + (e per x ma + basta) e
le parentesi; ma che consenta di avere il numero di parentesi aperte e non ancora chiuse al più 2. Questo vincolo,
costante, permette di usare un linguaggio regolare. (Riesco a tenere traccia delle parentesi con un numero finito di
stati).

Proviamo a scrivere un e.r. o un AFD.
%TODO

> Le espressioni regolari non sono lo strumento giusto per descrivere vincoli sintattici.

## Context-free

La classe di linguaggi immediatamente più ricca rispetto alle e.r. sono i linguaggi liberi da contesto. Adesso riusciamo
ad esprimere i vincoli di cui sopra: possiamo descrivere la sintassi dei linguaggi di programmazione con i linguaggi
context-free, ma non tutte le caratteristiche sono esprimibili così.

Per riconoscere se ho già definito una variabile (implicitamente o esplicitamente), non posso usare un linguaggio
context-free. Altre caratteristiche sono che il numero di argomenti sia uguale al numero di parametri formali della
firma della funzione.

Scrivere un linguaggio c.f. è semplice, la maggior parte delle strutture sono c.f. e alcune caratteristiche dipendenti
dal contesto vengono trattate nella fase di analisi semantica.

La grammatica del C è uno standard ISO.

[Rust grammar](https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/grammar.html)

## Grammatica

Una grammatica è un formalismo per definire linguaggi.
È un formalismo generativo (cioè mediante la grammatica è possibile generare tutte e sole le frasi del linguaggio).

Il parser può usare la grammatica in due modi:

- parsing top-down, partendo da un assioma fondamentale della grammatica e derivare la frase. Ad esempio nel C l'assioma
fondamentale è `un-programma :` (può essere).

- parsing bottom-up: si procede a ritroso cercando di ritornare all'assioma base.

> Una grammatica è una quadrupla di elementi $G = (\mathcal{N}, \mathcal{T}, \mathcal{P}, \mathcal{S})$. Dove N è
l'insieme dei simboli non terminali (che giocano un ruolo intermedio strutturale), mentre T è un insieme di simboli
terminali. S è il simbolo iniziale (o assioma) e P è l'insieme delle produzioni, cioè scritture nella forma $X
\to Y$ dove $X, Y \in (\mathcal N \cup \mathcal T).

Le produzioni vengono chiamate regole di riscrittura perché dovunque troviamo X allora possiamo sostituirlo con Y. È
questa la grande capacità espressiva delle grammatiche context-free.

### Struttura delle produzioni

$A \to xB$ oppure $A \to x$ è una grammatica lineare destra, mentre $A \to Bx$ è una grammatica
lineare sinistra.

Un linguaggio libero è $A \to \alpha$, ovvero posso sostituire un non terminale con una sequenza di terminali.

Definita una grammatica (vedi slide 20), come fa a descrivere un linguaggio? È di tipo generativo quindi applico
ripetutamente le regole di riscrittura, detta **derivazione**.

> La derivazione è una sequenza di applicazione delle produzioni che fanno passare dall'assioma a una sequenza di soli
terminali; ottenendo una frase del linguaggio.

Si indica con $\Rightarrow$ perché altrimenti avrei ambiguità con la notazione delle produzioni $\to$. Riusciamo a
caratterizzare l'intero linguaggio? Questa genera `a*b*` e si può dimostrare.

Le grammatiche lineari equivalgono alle espressioni regolari.

> Il linguaggio generato da una grammatica è il linguaggio di tutte le sequenze di terminali che possono essere derivate
dall'assioma iniziale.

Le derivazioni si possono rappresentare tramite albero, non in modo biunivoco. I parser bottom-up sono i più potenti e
usati realmente nei parser dei linguaggi.

## Descrizione

Possiamo descrivere una grammatica in modo succinto descrivendo le produzioni direttamente, che contengono tutti e 4 gli
elementi delle grammatiche.

I simboli non terminali compaiono tutti sulla sinistra, un simbolo terminale non può comparire sia in testa che in coda.

Si può fare un'ulteriore semplificazione introducendo un metasimbolo che permette di raggruppare produzioni con la
stessa testa, ovvero `|`.

### Esempi

Consideriamo un'altra grammatica: $E \to E + E | E \times E | (E) | number$. C'è un solo non terminale: $E$.
Ci sono cinque token: $number$, +, $\times$, (, ).

Una frase è una sequenza di simboli **solo terminali**. Una forma di frase può contenere simboli non terminali. Nella
derivazione sono chiaramente tutte forme di frase fino alla penultima.

Dopo un numero di applicazioni (2) puoi ottenere la frase $number + number$: $E + E \Rightarrow ^* _{G_{expr1}}
\text{number} + \text{number}$.

Voglio derivare $n + n \times (n + n)$, allora faccio

$$
E \Rightarrow E + E \Rightarrow E + E \times E \Rightarrow E + E \times (E) \Rightarrow E + E \times (E + E)
\Rightarrow ^* n + n \times (n + n)
$$

Le derivazioni alternative non sono una cosa buona... la cosa ottimale sarebbe avere una sola derivazione per ogni frase
perché legata al concetto di derivazione (e albero) c'è una prima bozza del significato di una frase.

Posso fare la derivazione con il $\times$. Questa grammatica è ambigua.

La derivazione giusta è quella con il $+$ come derivazione iniziale del parse tree. È un concetto distinto dall'abstract
syntax tree però lo genera, quindi applicando subito la regola $\times$ avrei la moltiplicazione come prima operazione.

Data una grammatica, dimostrare che genera un linguaggio, non è banale e si fa per induzione. Si congettura la forma e
bisogna generare che se una stringa è generata allora ha la particolare forma congetturata e viceversa. Ma questo esula
dai nostri scopi.

## Liberi dal contesto

Un linguaggio si chiama libero (tipica domanda da orale), proprio perché le produzioni, in qualsiasi punto della
derivazione se compare la testa della produzione allora posso sostituirla con la produzione indipendentemente da quello
che sta a destra e quello che sta a sinistra.

Mentre nelle grammatiche context-dependent abbiamo due "carabinieri": $\beta A \gamma \to \beta \alpha \gamma$, cioè
posso riscrivere A con $\alpha$ solo se mi trovo tra $\beta$ e $\gamma$.

Il **contesto** fornisce informazioni sulla definizione della funzione (può essere una stringa molto grande).
Pur avendo le grammatiche context-dependent non si usano, ma si fanno controlli semantici algoritmici. Ragione profonda
per cui non si usano.

Sono tutte cose che possiamo inserire nella definizione sintattica, al costo di complicare inultimente la grammatica.

## Gerarchia di Chomksy

Definisce la progressione delle grammatiche in base alla complessità e alla potenza richiesta, dal tipo 3 al tipo 0.

Una grammatica context-free (tipo 2) richiede solo un'automa a stati finiti e uno stack.

Le grammatiche di tipo 1, dipendenti dal contesto, si riconoscono con automi limitati linearmente, quindi con memoria
ad accesso casuale (posso anche accedere sequenzialmente ma devo poter accedere sempre a tutto).

Infine le grammatiche ricorsive si riconoscono con le macchine di Turing.

[Linguaggio ricorsivo](https://it.wikipedia.org/wiki/Linguaggio_ricorsivo)

## Riconoscere parentesi

Scrivere una grammatica per riconoscere sequenze di parentesi. Ad esempio: `()((()()))`

I simboli terminali sono $\mathcal T = \{(, )\}$.

$$
A \to \epsilon \\
A \to (A) \\
A \to AA
$$

che si può semplificare in

$$
A \to \epsilon | (A)A
$$
