# Espressioni regolari [Lexer]

Le espressioni regolari su un alfabeto $\Sigma$ sono un formalismo che serve a costruire linguaggi regolari.
La definizione di espressione regolare è ricorsiva:

- base:
    - $\epsilon$ è un'espressione regolare che denota il linguaggio composto dalla sola stringa vuota;
    - per ogni $a \in \Sigma$, a è un'espressione regolare che denota il linguaggio unitario {a}.

- ricorsione, se $\mathcal{E}$ e $\mathcal{F}$ sono due espressioni regolari che denotano i linguaggi E ed F, allora:
    - $\mathcal{E}\mathcal{F}$ denota una concatenazione dei due linguaggi;
    - $\mathcal{E} | \mathcal{F}$ denota l'unione;
    - $\mathcal{E}*$ denota la chiusura riflessiva.

Facciamo un esempio sull'alfabeto $\mathcal{B}$: `0|1*10` che dalle regole di precedenza si interpreta come `0|((1*)10)`
e denota il linguaggio R={0, 10, 110, 1110, ...}, ovvero le stringhe terminano con 0 e possono avere degli 1 davanti.

`(1|0)*(0|1|01)` denota il linguaggio di lunghezza almeno 1 che non contiene due caratteri 0 consecutivi.
La parte `(0|1|01)` è la base mentre `(1|0)*` è ricorsiva. Dimostra che l'espressione matcha tutte e sole le stringhe
senza zeri consecutivi.

### Dimostrazione (TODO)

## Limiti

Nelle espressioni regolari base non è possibile scrivere la negazione. È un problema pratico se vogliamo scrivere, ad esempio una e.r. che denoti le stringhe sull'alfabeto ASCII che non terminano con `1`. La soluzione, come e.r. pura, è scrivere una e.r. che denota una stringa che termina per qualsiasi carattere, tranne l'1 $\Sigma^*$ `(a|b|c|d|...)`.

## Espressioni regolari estese

Non riconoscono linguaggi più larghi dei linguaggi regolari (che sono inclusi propriamente nei linguaggi context-free),
sono semplicemente più sintetiche e permettono di esprimere il NOT (ad esempio). Le e.r. estese vanno pensate come delle
macro che vanno espanse. `not 1` viene espansa come `(2|3|4|...)`.

## Metacaratteri

Alcune scritture abbreviano gruppi di carattere, come `[:alpha:]` che denota l'insieme dei caratteri alfabetici del
locale corrente. `[:digit:]` fa il match delle dieci cifre e `[:alnum:]` denota l'insieme dei caratteri alfabetici e
numerici.

Per fare match con `[]` entrano in gioco gli escape.

Posso anche costruire dei metacaratteri personalizzati con `[]` e i caratteri all'interno sono in **unione** quindi in
OR, ad esempio `[_[:alpha:]]` denota l'insieme dell'underscore e di un carattere alfabetico.

Se metto `^` (caret) allora denoto la negazione del metacarattere che specifico dopo. `[^[:alnum:]]` denota l'insieme
dei caratteri che non sono alfanumerici.

La scrittura $\mathcal E ?$ denota l'insieme ${\epsilon} \cup L(\mathcal E)$. Mentre $\mathcal E \{n, m\}$ denota
$L(\mathcal E)^n L(\mathcal E)^{n+1} ... L(\mathcal E)^m$.

L'operatore di chiusura non riflessiva è permesso nelle e.r. estese e si denota con +.

Se è definito un ordinamento fra i caratteri dell'alfabeto, allora si possono utilizzare convenzioni specifiche per
intervalli di caratteri. Ad esempio `[a-f]` denota i caratteri "compresi" fra a ed f.

Match di un identificatore Python:

```
[_[:alpha:]][_[:alnum]]*
```

l'underscore singolo è usato in Python se la variabile non ha alcun ruolo, ad esempio nei cicli for dove svolge il ruolo
di mero contatore.

## Usi

Solitamente le e.r. vengono usate, insieme a un testo di input, per trovare tutte le sottostringhe che matchano
l'espressione regolare; ad esempio `grep`.

In un frontend di un compilatore, il lexer riceve in input il testo e un insieme di espressioni regolari. Il lexer deve
"dire" quale espressione regolare descrive un opportuno prefisso del testo ancora da analizzare.

C'è una priorità nelle epsressioni regolari, ad esempio `formula = 2*x**2` deve riconoscere `formula` e non un for.

### Esercizi di matching (TODO)

Slide 19 e 20 di `espressioni_regolari.pdf`.

## Token

Un Token è un oggetto astratto caratterizzato da due attributi: un nome obbligatorio e un valore opzionale.

Il _token name_ è un identificatore che può essere arbitrario e deve essere lo stesso per lexer e parser. Nomi per i token possono essere `id`, `number`, `literal` e il valore invece è il "nome" riconosciuto dal lexer.

Ad esempio in `somma = 0` il lexer deve restituire la sequenza di token:

- `(id, "somma")`
- `assignment`
- `(number, 0)`

Tipicamente le keyword sono token a sè stanti.

## Lex: un lexical analyzer

Lex è un generatore di analizzatori lessicali, cioè che è in grado di generare un altro programma che riconosce stringhe
di un linguaggio regolare. Il nuovo programma è pensato per fare scanning e per fornire un output pronto da dare in
pasto a un parser.

Un generico programma in Lex contiene tre sezioni separate dalla sequenza `%%`.

```lex
Dichiarazioni
%%
Regole di traduzione
%%
Funzioni ausiliarie
```

La sezione "dichiarazioni" può contentere definizione di costanti/variabili, header e _definizioni regolari_ cioè r.e.
con un nome.

La sezione delle regole di traduzione sono le descrizioni delle azioni che devono essere eseguite a seguito del
riconoscimento dell'istanza di un pattern.

Nelle funzioni ausiliare aggiungiamo un main, perché Lex/Flex di suo non fornisce un entry point visto che solitamente
il lexer viene chiamato come subroutine dal lexer.

Per compilare:

```sh
flex -+ -o simpletok.cc simpletok.l
```

dove l'opzione `-+` che può sembrare strana specifica che vogliamo un sorgente C++. `-o` specifica il file di output.
I programmi generati prendono il loro input da stdin.

`yytext` è la variabile di tipo string in cui il lexer mette il valore matchato.

> TODO Riconosci gli altri operatori aritmetici, gli identificatori, le due parentesi e la parola chiave for

Ricorda che le espressioni regolari cercano sempre di matchare con il lessema più lungo.
