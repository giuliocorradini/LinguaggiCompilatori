# Espressioni regolari [Lexer]

Le espressioni regolari su un alfabeto $\Sigma$ sono un formalismo che serve a costruire linguaggi regolari.
La definizione di espressione regolare è ricorsiva:

- base:
    - $\epsilon$ è un'espressione regolare che denota il linguaggio composto dalla sola stringa vuota;
    - per ogni $a \in \Sigma$, a è un'espressione regolare che denota il linguaggio unitario {a}.

- ricorsione, se $\mathcal{E}$ e $\mathcal{F}$ sono due espressioni regolari che denotano i linguaggi E ed F, allora:
    - $\mathcal{E}\mathcal{F}$ denota una concatenazione dei due linguaggi;
    - $\mathcal{E} | \mathcal{F}$ denota l'unione;
    - $\mathcal{E}*$ denota la chiusura riflessiva.

Facciamo un esempio sull'alfabeto $\mathcal{B}$: `0|1*10` che dalle regole di precedenza si interpreta come `0|((1*)10)`
e denota il linguaggio R={0, 10, 110, 1110, ...}, ovvero le stringhe terminano con 0 e possono avere degli 1 davanti.

`(1|0)*(0|1|01)` denota il linguaggio di lunghezza almeno 1 che non contiene due caratteri 0 consecutivi.
La parte `(0|1|01)` è la base mentre `(1|0)*` è ricorsiva. Dimostra che l'espressione matcha tutte e sole le stringhe
senza zeri consecutivi.

### Dimostrazione

La dimostrazione che la e.r. genera solo stringhe senza zeri consecuitivi si effettua considerando il diagramma delle
transizioni della macchina a stati equivalente per la regex e si vede come gli unici nodi finali validi siano
raggiungibili solo se non si prende due volte un ramo 0, che altrimenti conduce a un nodo pozzo.
In particolare lo stato 'b' è lo stato di quando si incontra almeno uno 0.

Per dimostrare che l'e.r. genera tutte le stringhe senza zeri consecuitivi bisogna riscrivere la tesi così:

L'.e.r. M genera tutte le stringhe senza senza zeri consecuitivi $\Leftrightarrow$ $\nexists$ una stringa s senza zeri
consecutivi tale che $l \notin L(M)$ (L(M) è il linguaggio generato dalla e.r.).

Questo è vero $\Leftrightarrow$ $\nexists$ s senza zeri consecuitivi tale che $s \in \overline L (M)$.
Infatti se una stringa non sta nel linguaggio, allora sta nel complementare per definizione.

Si valuta quindi com'è composto il linguaggio $\overline L(M)$, la cui macchina a stati prevede che le uniche stringhe
valide siano la stringa vuota e quella denotata dal nodo d, che si raggiunge scansionando due zeri consecutivi.

Quindi tutte le stringhe del linguaggio complementare sono o la stringa vuota, oppure le stringhe che hanno almeno una
coppia di zeri al loro interno. Cioè:

$\forall s \in \overline L(M)$, s ha due zeri consecutivi $\Leftrightarrow$ $\notexists$ s senza zeri consecuitivi:
$s \in \overline L(M)$.

da cui la tesi.

## Limiti

Nelle espressioni regolari base non è possibile scrivere la negazione. È un problema pratico se vogliamo scrivere, ad esempio una e.r. che denoti le stringhe sull'alfabeto ASCII che non terminano con `1`. La soluzione, come e.r. pura, è scrivere una e.r. che denota una stringa che termina per qualsiasi carattere, tranne l'1 $\Sigma^*$ `(a|b|c|d|...)`.

## Espressioni regolari estese

Non riconoscono linguaggi più larghi dei linguaggi regolari (che sono inclusi propriamente nei linguaggi context-free),
sono semplicemente più sintetiche e permettono di esprimere il NOT (ad esempio). Le e.r. estese vanno pensate come delle
macro che vanno espanse. `not 1` viene espansa come `(2|3|4|...)`.

## Metacaratteri

Alcune scritture abbreviano gruppi di carattere, come `[:alpha:]` che denota l'insieme dei caratteri alfabetici del
locale corrente. `[:digit:]` fa il match delle dieci cifre e `[:alnum:]` denota l'insieme dei caratteri alfabetici e
numerici.

Per fare match con `[]` entrano in gioco gli escape.

Posso anche costruire dei metacaratteri personalizzati con `[]` e i caratteri all'interno sono in **unione** quindi in
OR, ad esempio `[_[:alpha:]]` denota l'insieme dell'underscore e di un carattere alfabetico.

Se metto `^` (caret) allora denoto la negazione del metacarattere che specifico dopo. `[^[:alnum:]]` denota l'insieme
dei caratteri che non sono alfanumerici.

La scrittura $\mathcal E ?$ denota l'insieme ${\epsilon} \cup L(\mathcal E)$. Mentre $\mathcal E \{n, m\}$ denota
$L(\mathcal E)^n L(\mathcal E)^{n+1} ... L(\mathcal E)^m$.

L'operatore di chiusura non riflessiva è permesso nelle e.r. estese e si denota con +.

Se è definito un ordinamento fra i caratteri dell'alfabeto, allora si possono utilizzare convenzioni specifiche per
intervalli di caratteri. Ad esempio `[a-f]` denota i caratteri "compresi" fra a ed f.

Match di un identificatore Python:

```
[_[:alpha:]][_[:alnum]]*
```

l'underscore singolo è usato in Python se la variabile non ha alcun ruolo, ad esempio nei cicli for dove svolge il ruolo
di mero contatore.

## Usi

Solitamente le e.r. vengono usate, insieme a un testo di input, per trovare tutte le sottostringhe che matchano
l'espressione regolare; ad esempio `grep`.

In un frontend di un compilatore, il lexer riceve in input il testo e un insieme di espressioni regolari. Il lexer deve
"dire" quale espressione regolare descrive un opportuno prefisso del testo ancora da analizzare.

C'è una priorità nelle epsressioni regolari, ad esempio `formula = 2*x**2` deve riconoscere `formula` e non un for.

### Esercizi di matching (TODO)

Slide 19 e 20 di `espressioni_regolari.pdf`.

## Token

Un Token è un oggetto astratto caratterizzato da due attributi: un nome obbligatorio e un valore opzionale.

Il _token name_ è un identificatore che può essere arbitrario e deve essere lo stesso per lexer e parser. Nomi per i token possono essere `id`, `number`, `literal` e il valore invece è il "nome" riconosciuto dal lexer.

Ad esempio in `somma = 0` il lexer deve restituire la sequenza di token:

- `(id, "somma")`
- `assignment`
- `(number, 0)`

Tipicamente le keyword sono token a sè stanti.

## Lex: un lexical analyzer

Lex è un generatore di analizzatori lessicali, cioè che è in grado di generare un altro programma che riconosce stringhe
di un linguaggio regolare. Il nuovo programma è pensato per fare scanning e per fornire un output pronto da dare in
pasto a un parser.

Un generico programma in Lex contiene tre sezioni separate dalla sequenza `%%`.

```lex
Dichiarazioni
%%
Regole di traduzione
%%
Funzioni ausiliarie
```

La sezione "dichiarazioni" può contentere definizione di costanti/variabili, header e _definizioni regolari_ cioè r.e.
con un nome.

La sezione delle regole di traduzione sono le descrizioni delle azioni che devono essere eseguite a seguito del
riconoscimento dell'istanza di un pattern.

Nelle funzioni ausiliare aggiungiamo un main, perché Lex/Flex di suo non fornisce un entry point visto che solitamente
il lexer viene chiamato come subroutine dal lexer.

Per compilare:

```sh
flex -+ -o simpletok.cc simpletok.l
```

dove l'opzione `-+` che può sembrare strana specifica che vogliamo un sorgente C++. `-o` specifica il file di output.
I programmi generati prendono il loro input da stdin.

`yytext` è la variabile di tipo string in cui il lexer mette il valore matchato.

> TODO Riconosci gli altri operatori aritmetici, gli identificatori, le due parentesi e la parola chiave for

Ricorda che le espressioni regolari cercano sempre di matchare con il lessema più lungo.

### Diving into Flex

`%option noyywrap c++` fa si che il controllo di flusso venga restituito al chiamante di `->yylex()` appena terminato
il lexing dello stream corrente; se questa direttiva non è presente, viene chiamata la funzione `yywrap` che permette
di riavviare automaticamente lo scan per fare il lexing di più file di input.

Per gestire più file posso tenere una container `files`, una list of strings da cui a ogni chiamata estraggo un valore
che passo al lexer.
Il contenitore viene popolato nel main a partire dagli argomenti passati dalla riga di comando.

Ci sono tre funzioni per riavviare il lexer: `switch_streams(std::istream &)`, `yyrestart(std::istream &)` e `yylex()`.

Flex può essere usato per compiti diversi ma dalla stessa natura algoritmica rispetto a un lexer, ad esempio contare
righe, parole e caratteri come fa `wc`.

### Implementazione di wc in Flex

```lex
word [^ \t\n]+
eol \n

%%

{word} {wordcount++; charcount+=yyleng;}
{eol} {charcount++; linecount++;}
. {charcount++;}

%%

int wordcount = 0;
int linecount = 0;
int charcount = 0;
```

`yytext` è una stringa col testo matchato, `yyleng` è la corrispondente lunghezza del match.

Per sviluppare un parser dobbiamo richiamare il lexer e abbiamo bisogno di un header per la definizione della classe
ovvero andiamo a includere `<FlexLexer.h>` che è presente nel PATH delle librerie di sistema (almeno su macOS).

Per compilare solo il lexer come file oggetto:

```sh
clang++ -c wc.o wc.cpp
```

dove wc.cpp è stato generato da Flex (senza specificare un main all'interno).

L'opzione `-c` arriva a generare un file oggetto a partire dal sorgente, in particolare esegue il preprocessore, il
parser, il type checking, il generatore di IR e infine l'assemblatore. Infatti da `man clang`:

```
OPTIONS
   Stage Selection Options

       -E     Run the preprocessor stage.

       -fsyntax-only
              Run the preprocessor, parser and type checking stages.

       -S     Run the previous stages as well as LLVM generation and optimization stages and target-specific code generation, producing an assembly file.

       -c     Run all of the above, plus the assembler, generating a target ".o" object file.
```

Adesso devo linkare dal main: definisco extern le variabili globali e includo FlexLexer.

```sh
# Compilo il main, come file oggetto
clang++ -c wcmain.o wcmain.cpp

# Chiama solo il linker
clang++ -o wcmain wcmain.o wcs.o
```

Noi stiamo lavorando su un linguaggio eminentemente funzionale, ma con qualche aspetto di gestione della memoria:
Kaleidoscope; studiato e proposto dagli sviluppatori di LLVM come linguaggio giocattolo per prendere dimestichezza con
il compilatore.

Useremo Bison per la grammatica invece di scrivere gli strumenti _from scratch_ come nella guida di LLVM.

Il linguaggio Kaleidoscope accetta:

```
def g(x y z) x*y+z;
extern f(x y);
3*f(3,4)-x;
```

TODO: scrivere il corrispondente lexer come esercizio

Come azione rispetto a un pattern matchato posso lanciare un'azione, ad esempio {ide} {check_ide(yytext);}